<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Learning R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Elizabeth Oneita Davis" />
    <meta name="date" content="2020-03-05" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Learning R
## It doesn’t have to be scary!
### Elizabeth Oneita Davis
### San Diego Zoo Global
### 2020-03-05

---




&lt;style type="text/css"&gt;
/* custom.css */
.left-code {
  color: #777;
  width: 38%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 60%;
  float: right;
  padding-left: 1%;
}
.plot-callout {
  height: 225px;
  width: 450px;
  bottom: 5%;
  right: 5%;
  position: absolute;
  padding: 0px;
  z-index: 100;
}
.plot-callout img {
  width: 100%;
  border: 4px solid #23373B;
}
&lt;/style&gt;

---
# Getting Data into R
Go to the following site:

https://vincentarelbundock.github.io/Rdatasets/datasets.html 

--

and download the dataset “survival”. We will read this into R using the following command:


```r
Hanoi&lt;-read.csv("survival.csv")
```

--

When you read this into R, you should see the following:

---

![survival dataset](images/survival-dataset.png)

---

![hanoi title](images/hanoi-title.png)

---

As you can see, we use the arrow to assign our dataset to a name. Therefore,when you type "Hanoi" it will show you the dataset that you have loaded into R. 

--

However, R Studio makes things very simple by cutting out this extra step. If you look at R Studio, you’ll note that in the top right corner there’s a box that has the tab *“Environment”*. 

You’ll see that there’s a button that says *“Import dataset”*. You can use this button to quickly import any dataset you desire. Try it now with the "survival" dataset you just imported into R as “Hanoi”.

---

class: center, inverse
background-image: url("images/peaceful-fields.png")

---

# A quick, basic lesson

--

Before we go any further, I must stress that R is very, very particular. If a command goes wrong, it is 99% probable that you did not write the name of the dataset/vector/variable properly, OR that you did not put a parentheses or a comma, or some other form of punctuation where there should be one. The beauty of R studio is that it’s very good at putting parentheses in the right places, but errors still do occur.

--

R is quite simply, an entirely new language. That being said, it operates in an intuitive manner. For instance, try this command:


```r
1+1
```

If you press enter, you'll see that it executes the code and gives you the answer, which is


```
## [1] 2
```

--

Of course, we all know what 1 + 1 is (but imagine if we didn’t!).

---

# Assigning equations to vectors

Now, what if we wanted to assign the value of the equation **1+1** to an object in R? 

.pull-left[

```r
a &lt;- 1+1
a
```
]
--

.pull-right[

```
## [1] 2
```
]

--

You can see that the value of this equation is stored in the object of *a*. Now, perhaps you can begin to see what kinds of things you can do with this object, e.g.

--
.pull-left[

```r
a + 56
```
]

--
.pull-right[

```
## [1] 58
```
]

---
class: center, middle

# R is simple enough to understand if you think of it as small **building blocks**, such as `a &lt;- 1+1`, being put together to comprise a whole (the action you are trying to accomplish).

--

### This may not make sense right now, but (hopefully!) will by the end of this course.

---

# Taking a look at our dataset

--

Before starting analysis on a dataset, you need to familiarize yourself with the data. This can be done in R very easily. One command that's useful is the following, on our "Hanoi" dataset:

--

.pull-left[

```r
head(Hanoi)
```
]
--

.pull-right[

```
##   X  dose  surv
## 1 1 117.5 44.00
## 2 2 117.5 55.00
## 3 3 235.0 16.00
## 4 4 235.0 13.00
## 5 5 470.0  4.00
## 6 6 470.0  1.96
```
]

--

As you can see, this shows you the top of the dataset (the "head", if you will), which is helpful if one wants to get a quick snapshot of the dataset. 

---

The next command is:

--

.pull-left[

```r
names(Hanoi)
```
]
--

.pull-right[

```
## [1] "X"    "dose" "surv"
```
]

--

Our variables here are "dose" and "survival". To give you some context, this dataset is about the survival of rats after being given radiation doses. "surv" is the survival rate of the batches expressed as a percentage, while "dose" is the dose of radiation administered (rads).

--

Now, it doesn't seem to make sense for our dataset to be called "Hanoi". How do you think we would go about renaming it?

--


```r
survival &lt;- Hanoi
```

Now, you should see in your top right pane, under "Environment", the dataset `survival`, which is exactly the same as the `Hanoi` dataset.

---

# Summary statistics in R

--

This is very easy to do in R. Simply type the following:

--


```r
summary(survival)
```


--



```
##        X              dose             surv        
##  Min.   : 1.00   Min.   : 117.5   Min.   : 0.0060  
##  1st Qu.: 4.25   1st Qu.: 293.8   1st Qu.: 0.1625  
##  Median : 7.50   Median : 587.5   Median : 1.3300  
##  Mean   : 7.50   Mean   : 654.6   Mean   :10.1250  
##  3rd Qu.:10.75   3rd Qu.: 940.0   3rd Qu.:11.2800  
##  Max.   :14.00   Max.   :1410.0   Max.   :55.0000
```

--

As you can see, this command gives you a nice little snapshot of the data, with means and medians. You can also find the mean for each specific variable by simply typing the following:

.pull-left[

```r
mean(survival$dose)
```
]

--

.pull-right[

```
## [1] 654.6429
```
]

---

##As you see, this mean is the same as the mean we see in the summary statistics we did with `summary(survival)`.

---

## You probably noticed the use of the "$" operator. This is your way of telling R where to "look". When you type **mean(survival$dose)** you are telling R "I want the mean of the variable *dose* in the *survival* dataset."

--

## Now on your own, try finding the **median** of "surv"

--

You should have typed in the following, and gotten the following result:

--

```r
median(survival$surv)
```

--


```
## [1] 1.33
```


---

&lt;img src="images/bear-vinh.png" width="1400px" /&gt;

---

# How do you make sense of your data?

--

## Traditional Tools  

--

In earlier versions of this lecture, I would tell students to investigate **three key assumptions** (namely, normaility, homogeneity of variances, and whether their samples were indpendent).

--

Nowadays, I don't think those are as important. You absolutely should understand your data, but for social scientists our data is 99%&lt;sup&gt;1&lt;/sup&gt; of the time going to be non-normal, and often with wide variances. And as social scientists, we should already know whether our data is dependent or indpendent.

--

.footnote[
[1] Definitely not accurate math.

]

---

### In the old days, we used null hypothesis significance tests (NHST)

--

### These are now rightly criticized for obscuring the "true" effect we can see, and/or being a "lazy" way to interrogate data. Also, there are plenty of instances where it is simply unneccesary to perform NHSTs.

--

### e.g....

---

![danielle-tweet](images/danielle-tweet.png)

---




---

We can also visualize this with a histogram of the data, using the `hist` command:

--


```r
hist(survival$surv)
```

![](2019_11_June-learn-R-prez_files/figure-html/11-1.png)&lt;!-- --&gt;

---

As you can see, the data really aren’t normally distributed. In fact, the data are right-skewed (i.e. a long right tail).
--

We can also test for **homogeneity of variance** in R. It's done the following way:

--



```r
bartlett.test(survival$surv ~ survival$dose)
```

--



```
## 
## 	Bartlett test of homogeneity of variances
## 
## data:  survival$surv by survival$dose
## Bartlett's K-squared = 19.995, df = 5, p-value = 0.001253
```

--

This test shows us that the variances are also heterogenous in this dataset, so that assumption is also violated. Additionally, note that when you ran this test you did so using a new operator, the *tilda* i.e. *~*. This tells R that the variable to the left (*survival$surv*) **depends** on *survival$dose*. In English, this would be **"Survival of rats depends on the radiation dose adminstered."**, i.e.

--

y ~ x

--

This is very important in R, and is how you will construct your arguments for statistical tests you will run!

---

## What did these tests for assumption tell us?&lt;sup&gt;1&lt;/sup&gt;

--

#### These tests have told us that we can't use the "traditional" statistical tests for significance of t-test or ANOVA. We have to use non-parametric statistical tests, such as Mann-Whitney U and Kruskal-Wallis.

--

But we will get to those later...

.footnote[
[1] If you remember, the third assumption is that the samples are independent of one another. As the researcher, it will be up to you to know/understand whether the samples are independent or not! 

]
 
---

# Statistical tests for significance in R

Go to the following page:
https://catalog.data.gov/dataset?res_format=CSV
and download the dataset called "NCHS - Death rates and life expectancy at birth". We will run some statistical tests on this data.

--

#### First, we load the data into R.

--


```r
NCHS &lt;- read.csv("NCHS_-_Death_rates_and_life_expectancy_at_birth.csv")
```

--

#### Next, we have a look at the data.

---



```r
head(NCHS)
```


--



```
##   Year      Race        Sex Average.Life.Expectancy..Years.
## 1 2015 All Races Both Sexes                              NA
## 2 2014 All Races Both Sexes                            78.9
## 3 2013 All Races Both Sexes                            78.8
## 4 2012 All Races Both Sexes                            78.8
## 5 2011 All Races Both Sexes                            78.7
## 6 2010 All Races Both Sexes                            78.7
##   Age.adjusted.Death.Rate
## 1                   733.1
## 2                   724.6
## 3                   731.9
## 4                   732.8
## 5                   741.3
## 6                   747.0
```


--

Interesting! (Well, to nerds like me.) Let's explore average life expectancy by gender. What test will we use to accomplish this?  

---

# Performing a t-test on our data

Unfortunately, with the data we have it won't be as straight-forward as it has been with running the code. However, if you follow the code I give you here, you should be fine! Don't worry too much about understanding it right now. As you get more comfortable with R, all of this will start to make sense. 
    
--

To get our data usable in a t-test, let's isolate the male and female values into one dataset using the following code:

--


```r
#install.packages("tidyverse") ## I have hashed this out for myself because I already have it installed. Do NOT put the hashtags in front of your command, when you install the package
#library(tidyverse)
NCHS_gender &lt;- NCHS %&gt;%
  dplyr::filter(Sex == "Male" | Sex == "Female") 
```

---
Now we need to pivot the data so that "Male" life expectancy is one group, and "Female" is another.


```r
NCHS_gender_wide &lt;- NCHS_gender %&gt;% spread(Sex, Average.Life.Expectancy..Years.)
```

--

Have a look at the data. You'll see that now there are two columns, one entitled "Female", and one titled "Male". These are what we will work with for the t-test.

---

### Performing the t-test

--

Check the data to see if it's normal, using the Shapiro-Wilk test.

--



```r
shapiro.test(NCHS_gender_wide$Female)
shapiro.test(NCHS_gender_wide$Male)
```


--



```
## 
## 	Shapiro-Wilk normality test
## 
## data:  NCHS_gender_wide$Female
## W = 0.90407, p-value = 5.699e-14
```

```
## 
## 	Shapiro-Wilk normality test
## 
## data:  NCHS_gender_wide$Male
## W = 0.92541, p-value = 4.269e-12
```


--

The p-value is significant in both cases, so we know that the data are NOT significant.

--- 

But let's perform a t-test anyway.

---



```r
t.test(NCHS_gender_wide$Female, NCHS_gender_wide$Male)
```


--



```
## 
## 	Welch Two Sample t-test
## 
## data:  NCHS_gender_wide$Female and NCHS_gender_wide$Male
## t = 5.8835, df = 679.17, p-value = 6.307e-09
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  3.464248 6.934592
## sample estimates:
## mean of x mean of y 
##  66.73536  61.53594
```


--

As you can see, you get a lot of information in this output. The p-value indicates that there IS significant difference between the two samples. The output also shows you the way the effect lies, i.e. it shows you that women tend to be older than men when they die, on average. 

---

## So, you could interpret this as "We are 95% confident that we can reject our null hypothesis that there is no difference in life expectancy between women and men (p&lt;0.05, df = 1)."

--

### However, as we know we should really have used a non-parametric test: the Mann-Whitney U test, or as R knows it, the *Wilcoxon* test.

--



```r
wilcox.test(NCHS_gender_wide$Female, NCHS_gender_wide$Male)
```


--


```
## 
## 	Wilcoxon rank sum test with continuity correction
## 
## data:  NCHS_gender_wide$Female and NCHS_gender_wide$Male
## W = 77826, p-value = 2.646e-12
## alternative hypothesis: true location shift is not equal to 0
```


---

### The Wilcoxon test tells us the same thing as the t-test, but you'll see the p-value is a little bit less significant. This is because the Wilcox test is more conservative with its calculations, compared to the t-test.

---

## REMEMBER: A t-test only calculates the rate of observable difference between TWO groups. 

--

### What test do we use for multiple groups?

---

# ANOVA

--

Let's play with a new dataset. Run the following command:

--


```r
data("InsectSprays")
```

--

### Let's take a moment to think about what we should do with a new dataset. What do we do first? What do we do second?

--

First, you will need to check your data out using `head`, `summary`, and other commands that you think might be helpful in figuring out what's going on.

--

Second, you will need to test your assumptions. Go ahead and do that now. 

--

(Remember, you will perform `shapiro.test` and `bartlett.test`.)

--

You'll have seen that the data is not normal and we don't have homogeneity of variances, but let's go ahead and do an ANOVA anyway.

---


```r
summary(aov(InsectSprays$count ~ InsectSprays$spray))
```


--



```
##                    Df Sum Sq Mean Sq F value Pr(&gt;F)    
## InsectSprays$spray  5   2669   533.8    34.7 &lt;2e-16 ***
## Residuals          66   1015    15.4                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


--

Hopefully, you all will have been able to interpret the result as telling us that once again we can reject our null hypothesis that there is no difference between the sprays. We can also visualize this, using the following command:

---


```r
plot(count ~ spray, data = InsectSprays)
```


--


![](2019_11_June-learn-R-prez_files/figure-html/box insect-out-1.png)&lt;!-- --&gt;

---
#Non-parametric ANOVA

--

## This is called a "Kruskal-Wallis test". To perform it, do the following:



```r
kruskal.test(InsectSprays$count ~ InsectSprays$spray)
```


--



```
## 
## 	Kruskal-Wallis rank sum test
## 
## data:  InsectSprays$count by InsectSprays$spray
## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10
```

--

Again, we see that the p-value is significant. 

---

# Important to note

--

## I have shown you two examples where the parametric test and the non-parametric show us significance, but there are plenty of instances where this is not the case, and if we perform a standard ANOVA without investigating the assumptions we may make a conclusion that is *not true*. It is very important to always choose the right test!

---

# Chi-squared

--

We are going to use the `cars` dataset in R. Run the following code:

--


```r
# Install the MASS package
#install.packages("MASS") ## I have hashed this out for myself because I already have it installed. Do NOT put the hashtags in front of your command, when you install the package.

# Load the library.
#library("MASS") #As before when you run this code make sure you don't use the hash!

# Create a data frame from the main data set.
car.data &lt;- data.frame(Cars93$AirBags, Cars93$Type)

# Create a table with the needed variables.
car.data = table(Cars93$AirBags, Cars93$Type) 
car.data
```


--



```
##                     
##                      Compact Large Midsize Small Sporty Van
##   Driver &amp; Passenger       2     4       7     0      3   0
##   Driver only              9     7      11     5      8   3
##   None                     5     0       4    16      3   6
```



---

## Now, perform the Chi-Squared test.

--



```r
chisq.test(car.data)
```

```
## Warning in chisq.test(car.data): Chi-squared approximation may be incorrect
```


--



```
## Warning in chisq.test(car.data): Chi-squared approximation may be incorrect
```

```
## 
## 	Pearson's Chi-squared test
## 
## data:  car.data
## X-squared = 33.001, df = 10, p-value = 0.0002723
```

--

You can see that the p-value result is significant. Thus, we can reject our null hypothesis.

---

## Something to note: chi-squared is neither parametric or non-parametric (it's what is called a **rank** test), which is why we didn't perform the usual tests on our data. 

---

# Final comments

--

### Google will be your best friend when learning R. I still have to Google things ALL the time!

--

### There are tons of free resources for learning R! For example, check out https://learningstatisticswithr.com/book/introR.html. 

---

class: center, middle

# Great job! You all now know the basics of performing tests for significance in R!

---

background-image: url("images/finalslide.jpg")
background-position: center
background-size: contain
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
